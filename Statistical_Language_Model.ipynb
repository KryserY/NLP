{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pengwei-Yang/NLP/blob/main/Statistical_Language_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nA_uGHieevZ"
      },
      "source": [
        "# Statistical Language Model (SLM)\n",
        "\n",
        "A statistical language model produces probability distributions over sequences of words. Given a sequence, say of length m, it assigns a probability $P(w_1, \\ldots, w_m)$ to the whole sequence. One model solution is to make the assumption that the probability distribution for a word depends only on the previous $n-1$ words. This is known as an n-gram model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfT2RsFvE3Lc"
      },
      "source": [
        "## Bigrams and Trigrams\n",
        "\n",
        "An n-gram model is a type of probabilistic language model for predicting the next token in  a sequence in the form of a (n − 1)–order Markov model. Using Latin numerical prefixes, an n-gram of size 1 is referred to as a \"unigram\"; size 2 is a \"bigram\"; size 3 is a \"trigram\". English cardinal numbers are sometimes used, e.g., \"four-gram\", \"five-gram\", and so on. Note, we typically only use language models that are a bigram or higher. A unigram model would be looking at the previou $n-1 = 0$ words! Looking at no words would make for a very poor model.\n",
        "\n",
        "The frequency distribution of every bigram in a string is commonly used for simple statistical analysis of text in many applications, including in computational linguistics, cryptography, speech recognition, and so on.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4yjU_UcstUy"
      },
      "source": [
        "Let's see how to build a such a model with NLTK. Let's download some Reuters data and inspect it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GJ6LDdq-dB6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a292fb8a-f41a-4278-a957-254ef91e8494"
      },
      "source": [
        "import nltk\n",
        "from nltk.util import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        "from nltk.corpus import reuters\n",
        "\n",
        "nltk.download('reuters')\n",
        "!unzip -qq /root/nltk_data/corpora/reuters.zip -d /root/nltk_data/corpora\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PmoQ501E_Tz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b249c821-7ca0-49fb-fd45-19489dc988ca"
      },
      "source": [
        "first_sentence = reuters.sents()[0]\n",
        "first_sentence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ASIAN',\n",
              " 'EXPORTERS',\n",
              " 'FEAR',\n",
              " 'DAMAGE',\n",
              " 'FROM',\n",
              " 'U',\n",
              " '.',\n",
              " 'S',\n",
              " '.-',\n",
              " 'JAPAN',\n",
              " 'RIFT',\n",
              " 'Mounting',\n",
              " 'trade',\n",
              " 'friction',\n",
              " 'between',\n",
              " 'the',\n",
              " 'U',\n",
              " '.',\n",
              " 'S',\n",
              " '.',\n",
              " 'And',\n",
              " 'Japan',\n",
              " 'has',\n",
              " 'raised',\n",
              " 'fears',\n",
              " 'among',\n",
              " 'many',\n",
              " 'of',\n",
              " 'Asia',\n",
              " \"'\",\n",
              " 's',\n",
              " 'exporting',\n",
              " 'nations',\n",
              " 'that',\n",
              " 'the',\n",
              " 'row',\n",
              " 'could',\n",
              " 'inflict',\n",
              " 'far',\n",
              " '-',\n",
              " 'reaching',\n",
              " 'economic',\n",
              " 'damage',\n",
              " ',',\n",
              " 'businessmen',\n",
              " 'and',\n",
              " 'officials',\n",
              " 'said',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thzu9ZHb7OvU"
      },
      "source": [
        "Now let's see what the n-grams look like. More details can be found at [bigrams()](https://www.nltk.org/api/nltk.util.html#nltk.util.bigrams),  [trigrams()](https://www.nltk.org/api/nltk.util.html#nltk.util.trigrams),  [ngrams()](https://www.nltk.org/api/nltk.util.html#nltk.util.ngrams)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97SmMUjnFKPc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec183f0f-c0ec-496e-d560-8292ddcf4c01"
      },
      "source": [
        "print(\"bigrams without padding: \", list(bigrams(first_sentence)))\n",
        "\n",
        "print(\"bigrams with padding: \", list(bigrams(first_sentence, pad_left=True, pad_right=True)))\n",
        "\n",
        "print(\"trigrams without padding: \", list(trigrams(first_sentence)))\n",
        "\n",
        "print(\"trigrams with padding: \", list(trigrams(first_sentence, pad_left=True, pad_right=True)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams without padding:  [('ASIAN', 'EXPORTERS'), ('EXPORTERS', 'FEAR'), ('FEAR', 'DAMAGE'), ('DAMAGE', 'FROM'), ('FROM', 'U'), ('U', '.'), ('.', 'S'), ('S', '.-'), ('.-', 'JAPAN'), ('JAPAN', 'RIFT'), ('RIFT', 'Mounting'), ('Mounting', 'trade'), ('trade', 'friction'), ('friction', 'between'), ('between', 'the'), ('the', 'U'), ('U', '.'), ('.', 'S'), ('S', '.'), ('.', 'And'), ('And', 'Japan'), ('Japan', 'has'), ('has', 'raised'), ('raised', 'fears'), ('fears', 'among'), ('among', 'many'), ('many', 'of'), ('of', 'Asia'), ('Asia', \"'\"), (\"'\", 's'), ('s', 'exporting'), ('exporting', 'nations'), ('nations', 'that'), ('that', 'the'), ('the', 'row'), ('row', 'could'), ('could', 'inflict'), ('inflict', 'far'), ('far', '-'), ('-', 'reaching'), ('reaching', 'economic'), ('economic', 'damage'), ('damage', ','), (',', 'businessmen'), ('businessmen', 'and'), ('and', 'officials'), ('officials', 'said'), ('said', '.')]\n",
            "bigrams with padding:  [(None, 'ASIAN'), ('ASIAN', 'EXPORTERS'), ('EXPORTERS', 'FEAR'), ('FEAR', 'DAMAGE'), ('DAMAGE', 'FROM'), ('FROM', 'U'), ('U', '.'), ('.', 'S'), ('S', '.-'), ('.-', 'JAPAN'), ('JAPAN', 'RIFT'), ('RIFT', 'Mounting'), ('Mounting', 'trade'), ('trade', 'friction'), ('friction', 'between'), ('between', 'the'), ('the', 'U'), ('U', '.'), ('.', 'S'), ('S', '.'), ('.', 'And'), ('And', 'Japan'), ('Japan', 'has'), ('has', 'raised'), ('raised', 'fears'), ('fears', 'among'), ('among', 'many'), ('many', 'of'), ('of', 'Asia'), ('Asia', \"'\"), (\"'\", 's'), ('s', 'exporting'), ('exporting', 'nations'), ('nations', 'that'), ('that', 'the'), ('the', 'row'), ('row', 'could'), ('could', 'inflict'), ('inflict', 'far'), ('far', '-'), ('-', 'reaching'), ('reaching', 'economic'), ('economic', 'damage'), ('damage', ','), (',', 'businessmen'), ('businessmen', 'and'), ('and', 'officials'), ('officials', 'said'), ('said', '.'), ('.', None)]\n",
            "trigrams without padding:  [('ASIAN', 'EXPORTERS', 'FEAR'), ('EXPORTERS', 'FEAR', 'DAMAGE'), ('FEAR', 'DAMAGE', 'FROM'), ('DAMAGE', 'FROM', 'U'), ('FROM', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.-'), ('S', '.-', 'JAPAN'), ('.-', 'JAPAN', 'RIFT'), ('JAPAN', 'RIFT', 'Mounting'), ('RIFT', 'Mounting', 'trade'), ('Mounting', 'trade', 'friction'), ('trade', 'friction', 'between'), ('friction', 'between', 'the'), ('between', 'the', 'U'), ('the', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.'), ('S', '.', 'And'), ('.', 'And', 'Japan'), ('And', 'Japan', 'has'), ('Japan', 'has', 'raised'), ('has', 'raised', 'fears'), ('raised', 'fears', 'among'), ('fears', 'among', 'many'), ('among', 'many', 'of'), ('many', 'of', 'Asia'), ('of', 'Asia', \"'\"), ('Asia', \"'\", 's'), (\"'\", 's', 'exporting'), ('s', 'exporting', 'nations'), ('exporting', 'nations', 'that'), ('nations', 'that', 'the'), ('that', 'the', 'row'), ('the', 'row', 'could'), ('row', 'could', 'inflict'), ('could', 'inflict', 'far'), ('inflict', 'far', '-'), ('far', '-', 'reaching'), ('-', 'reaching', 'economic'), ('reaching', 'economic', 'damage'), ('economic', 'damage', ','), ('damage', ',', 'businessmen'), (',', 'businessmen', 'and'), ('businessmen', 'and', 'officials'), ('and', 'officials', 'said'), ('officials', 'said', '.')]\n",
            "trigrams with padding:  [(None, None, 'ASIAN'), (None, 'ASIAN', 'EXPORTERS'), ('ASIAN', 'EXPORTERS', 'FEAR'), ('EXPORTERS', 'FEAR', 'DAMAGE'), ('FEAR', 'DAMAGE', 'FROM'), ('DAMAGE', 'FROM', 'U'), ('FROM', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.-'), ('S', '.-', 'JAPAN'), ('.-', 'JAPAN', 'RIFT'), ('JAPAN', 'RIFT', 'Mounting'), ('RIFT', 'Mounting', 'trade'), ('Mounting', 'trade', 'friction'), ('trade', 'friction', 'between'), ('friction', 'between', 'the'), ('between', 'the', 'U'), ('the', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.'), ('S', '.', 'And'), ('.', 'And', 'Japan'), ('And', 'Japan', 'has'), ('Japan', 'has', 'raised'), ('has', 'raised', 'fears'), ('raised', 'fears', 'among'), ('fears', 'among', 'many'), ('among', 'many', 'of'), ('many', 'of', 'Asia'), ('of', 'Asia', \"'\"), ('Asia', \"'\", 's'), (\"'\", 's', 'exporting'), ('s', 'exporting', 'nations'), ('exporting', 'nations', 'that'), ('nations', 'that', 'the'), ('that', 'the', 'row'), ('the', 'row', 'could'), ('row', 'could', 'inflict'), ('could', 'inflict', 'far'), ('inflict', 'far', '-'), ('far', '-', 'reaching'), ('-', 'reaching', 'economic'), ('reaching', 'economic', 'damage'), ('economic', 'damage', ','), ('damage', ',', 'businessmen'), (',', 'businessmen', 'and'), ('businessmen', 'and', 'officials'), ('and', 'officials', 'said'), ('officials', 'said', '.'), ('said', '.', None), ('.', None, None)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yvl2w18dB_Js"
      },
      "source": [
        "Now, let's build a trigram model using the Reuters corpus. Building a bigram model is completely analogous.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OBW2sORF1sn"
      },
      "source": [
        "# create a model which contains the trigram counts\n",
        "trigram_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "for sentence in reuters.sents():\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "        trigram_model[(w1, w2)][w3] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzZFpU5-GD66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75213b1b-677e-4bde-ea4d-18c71e8c626e"
      },
      "source": [
        "# inspect the counts of some trigrams\n",
        "\n",
        "print(\"count for 'what the economists':\", trigram_model[\"what\", \"the\"][\"economists\"])\n",
        "\n",
        "print(\"count for 'what the nonexistingword':\", trigram_model[\"what\", \"the\"][\"nonexistingword\"])\n",
        "\n",
        "# counts of the sentence starting with \"The\"\n",
        "print(\"count for 'the' starting a sentence:\", trigram_model[None, None][\"The\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count for 'what the economists': 2\n",
            "count for 'what the nonexistingword': 0\n",
            "count for 'the' starting a sentence: 8839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# counts of the sentence starting with \"The\"\n",
        "print(\"count for 'the' starting a sentence:\", trigram_model[\"what\", \"the\"].values())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKfBhSfSXio2",
        "outputId": "61041662-a3f2-4f9f-db5e-39046febbff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count for 'the' starting a sentence: dict_values([1, 1, 1, 2, 1, 1, 1, 1, 3, 3, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9AhXXaHGR-y"
      },
      "source": [
        "# convert counts to probabilities\n",
        "for w1_w2 in trigram_model:\n",
        "    total_count = float(sum(trigram_model[w1_w2].values())) # trigram前两词的出现总数\n",
        "    for w3 in trigram_model[w1_w2]:\n",
        "        trigram_model[w1_w2][w3] /= total_count # 三个词同时出现的次数/前两个词的出现总数"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9VxFRs3GVQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2fb447f-a9e1-4245-c863-253f154ede4a"
      },
      "source": [
        "print(trigram_model[\"what\", \"the\"][\"economists\"] )\n",
        "\n",
        "print(trigram_model[\"what\", \"the\"][\"nonexistingword\"])\n",
        "\n",
        "# probabilities of the sentence starting with \"The\"\n",
        "print(trigram_model[None, None][\"The\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.043478260869565216\n",
            "0.0\n",
            "0.16154324146501936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZalKlJptvh33"
      },
      "source": [
        "Now you have a tri-gram language model. Let's generate some text. The output text is actually really readable!\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_model[tuple([\"what\", \"the\"])].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVsFqL_keJsH",
        "outputId": "29627ff1-cf8a-4e7f-84b4-07909fc61385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['communique', 'chance', 'manager', 'central', 'announcement', 'minimum', 'administration', 'United', 'U', 'decision', 'Japanese', 'MPs', 'differential', 'British', 'Paris', 'state', 'Buffer', 'markets', 'Fed', 'company', 'offer', 'result', 'pressure', 'outcome', 'union', 'financial', 'government', 'parameters', 'economists', 'trade', 'chief', 'Administration', 'group', 'equity', 'State', 'conversation', 'the', 'nonexistingword'])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_model[tuple([\"what\", \"the\"])]['communique']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yo2CnN3lekCq",
        "outputId": "d72562ae-3f7d-4a39-9183-ebee84ad0397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.021739130434782608"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hBv-NscGe9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa588301-2c48-43a4-b48b-81c4558d8d77"
      },
      "source": [
        "import random\n",
        "\n",
        "for sample_no in range(4):\n",
        "    text = [None, None]\n",
        "\n",
        "    sentence_finished = False\n",
        "\n",
        "    # Keep generating the next word until reaching the end of the sentence\n",
        "    while not sentence_finished:\n",
        "        # Randomly select a probability threshold r\n",
        "        r = random.random()\n",
        "        accumulator = .0\n",
        "\n",
        "        # Go through the possible w3 conditioned on current w1 and w2\n",
        "        for word in trigram_model[tuple(text[-2:])].keys():\n",
        "            # Accumulate the probability\n",
        "            accumulator += trigram_model[tuple(text[-2:])][word]\n",
        "\n",
        "            # When the threshold is reached, use the current w3 as the next word to be generated\n",
        "            if accumulator >= r:\n",
        "                text.append(word)\n",
        "                break\n",
        "\n",
        "        # If the last two words are None, it will reach the end and stop generating\n",
        "        if text[-2:] == [None, None]:\n",
        "            sentence_finished = True\n",
        "\n",
        "    # The generated sentence is as follows\n",
        "    print(\"Sample output\", sample_no)\n",
        "    print(' '.join([t for t in text if t]))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample output 0\n",
            "BALL CORP & lt ; MXM > HOLDERS APPROVE DIXONS MERGER Dixons Group PLC > 1986 YEAR Group shr 24 cts vs 15 . 7 mln bales of cotton , metal can making operations this year to more expansive policies in the new capital is fairly small amounts , MCA ' s going to stay on the St Lambert lock here this afternoon .\n",
            "\n",
            "Sample output 1\n",
            "The yen ' s needs , the Commerce Ministry officials said .\n",
            "\n",
            "Sample output 2\n",
            "Standard gained 6 - 3 / 4 cts prior Pay April 15\n",
            "\n",
            "Sample output 3\n",
            "The smoothed deficit on invisibles was 234 mln reported a second stage of negotiations was \" totally unjustified .\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ9YDhpza29T"
      },
      "source": [
        "# Decoding Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgP7L9aowkD1"
      },
      "source": [
        "In NLP tasks such as dialogue, text summarization, and machine translation, the prediction required is a sequence of words.\n",
        "\n",
        "It is common for models developed for these types of problems to output a probability distribution over each word in the vocabulary for each word in the output sequence. It is then left to a decoder process to transform the probabilities into a final sequence of words.\n",
        "\n",
        "Decoding the most likely output sequence involves searching through all the possible output sequences based on their likelihood. The size of the vocabulary is often tens or hundreds of thousands of words, or even millions of words. Therefore, the search problem is exponential in the length of the output sequence and is intractable (NP-complete) to search completely.\n",
        "\n",
        "In practice, heuristic search methods are used to return one or more approximate or “good enough” decoded output sequences for a given prediction. In some special cases, we can develop methods that do not search the whole space explicitly, but do find the highest probability output nevertheless.\n",
        "\n",
        "Candidate sequences of words are scored based on their likelihood. It is common to use a greedy search or a beam search to locate candidate sequences of text. We will look at both of these decoding algorithms now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3q9CKB5arfc"
      },
      "source": [
        "## Greedy Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRs0dBDXv-3U"
      },
      "source": [
        "A simple approximation is to use a greedy search that selects the most likely word at each step in the output sequence. This approach has the benefit that it is very fast, but the quality of the final output sequences may be far from optimal.\n",
        "\n",
        "We can demonstrate the greedy search approach to decoding with a small contrived example in Python. We can start off with a prediction problem that involves a sequence of 10 words. Each word is predicted as a probability distribution over a vocabulary of 5 words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9bVjguBJksg"
      },
      "source": [
        "from numpy import array\n",
        "from numpy import argmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SthjwprJfaE"
      },
      "source": [
        "# define a sequence of 10 words over a vocab of 5 words\n",
        "data = [[0.01, 0.09, 0.3, 0.4, 0.2],\n",
        "        [0.4, 0.3, 0.2, 0.01, 0.09],\n",
        "        [0.01, 0.09, 0.3, 0.4, 0.2],\n",
        "        [0.4, 0.3, 0.2, 0.01, 0.09],\n",
        "        [0.01, 0.09, 0.3, 0.4, 0.2],\n",
        "        [0.4, 0.3, 0.2, 0.01, 0.09],\n",
        "        [0.01, 0.09, 0.3, 0.4, 0.2],\n",
        "        [0.4, 0.3, 0.2, 0.01, 0.09],\n",
        "        [0.01, 0.09, 0.3, 0.4, 0.2],\n",
        "        [0.4, 0.3, 0.2, 0.01, 0.09]]\n",
        "data = array(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82tjZn5SwHks"
      },
      "source": [
        "We will assume that the words have been integer encoded, such that the column index can be used to look-up the associated word in the vocabulary. Therefore, the task of decoding becomes the task of selecting a sequence of integers from the probability distributions.\n",
        "\n",
        "The argmax() mathematical function can be used to select the index of an array that has the largest value. We can use this function to select the word index that is most likely at each step in the sequence. This function is provided directly in numpy.\n",
        "\n",
        "The greedy_decoder() function below implements this decoder strategy using the argmax function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9o2t8bjJveU"
      },
      "source": [
        "# greedy decoder, pick only index for largest probability each row\n",
        "def greedy_decoder(data):\n",
        "    return [argmax(s) for s in data] # Argmax 会返回最大值对应的索引index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = [0.4, 0.3, 0.2, 0.01, 0.09]\n",
        "b = argmax(a)"
      ],
      "metadata": {
        "id": "J2M6X8OjDqsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEdm502jDwt0",
        "outputId": "3b8dd6a2-9988-4abf-ce6c-343f79f41921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ccJuMoYwVFR"
      },
      "source": [
        "Running the example outputs a sequence of integers that could then be mapped back to words in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbo4v9KeJ6um",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfc9bec3-1673-4e55-9e09-7e7afd640de9"
      },
      "source": [
        "#decode seqeunce\n",
        "result = greedy_decoder(data)\n",
        "print(result) #返回一系列索引，这里的words为数字表示"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 0, 3, 0, 3, 0, 3, 0, 3, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o46exoAnKN80"
      },
      "source": [
        "## Beam Search Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz5tkvgQw7_K"
      },
      "source": [
        "Another popular heuristic is beam search, which improves on greedy search and returns a list of most likely output sequences.\n",
        "\n",
        "Instead of greedily choosing the most likely next step as the sequence is constructed, beam search keeps the k most likely, where k is a user-specified parameter and controls the number of beams or parallel searches through the sequence of probabilities.\n",
        "\n",
        "We do not need to start with random states; instead, we start with the k most likely words as the first step in the sequence. Common beam width values are 1 for a greedy search and values of 5 or 10 for common benchmark problems in machine translation. Larger beam widths result in better performance of a model as the multiple candidate sequences increase the likelihood of better matching a target sequence. This increased performance results in a decrease in decoding speed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX8CkPBLKZ3Z"
      },
      "source": [
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from numpy import log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4qTw0jQxVJV"
      },
      "source": [
        "The beam_search_decoder() function below implements a beam search decoder."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = [[list(), 0.0]]\n",
        "for step,row in enumerate(data):\n",
        "        all_candidates = list()\n",
        "        # expand each current candidate\n",
        "        for i in range(len(sequences)):\n",
        "            print(sequences)\n",
        "            seq, score = sequences[i]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbcJ0MBaFpF0",
        "outputId": "f222ceee-1fd7-4d64-bf8f-8dbc10a7a17d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[], 0.0]]\n",
            "[[[], 0.0]]\n",
            "[[[], 0.0]]\n",
            "[[[], 0.0]]\n",
            "[[[], 0.0]]\n",
            "[[[], 0.0]]\n",
            "[[[], 0.0]]\n",
            "[[[], 0.0]]\n",
            "[[[], 0.0]]\n",
            "[[[], 0.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ5EFT3nKQSa"
      },
      "source": [
        "# beam search\n",
        "def beam_search_decoder(data, k):\n",
        "    sequences = [[list(), 0.0]]\n",
        "    # walk over each step in sequence\n",
        "    for step,row in enumerate(data):\n",
        "        all_candidates = list()\n",
        "        # expand each current candidate\n",
        "        for i in range(len(sequences)):\n",
        "            seq, score = sequences[i]\n",
        "            for j in range(len(row)):\n",
        "                candidate = [seq + [j], score + (-log(row[j])) ]  #we are summing up the negative log, so we need to find the minimum score(which is the highest prob)\n",
        "                all_candidates.append(candidate)\n",
        "        # order all candidates by score\n",
        "        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "\n",
        "        # select k best\n",
        "        sequences = ordered[:k]\n",
        "\n",
        "        # display the k-best sequences\n",
        "        print(\"The\", str(k), \"best sequences at step \", str(step), \": \")\n",
        "        print(sequences)\n",
        "        print()\n",
        "\n",
        "    return sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-Rro25Dxbu0"
      },
      "source": [
        "We can tie this together with the sample data from the previous section and this time return the 3 most likely sequences. Running the example prints both the integer sequences and their log likelihood."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO-UQOwuKUh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c0b360d-fbcd-413e-efae-41c8c104bdca"
      },
      "source": [
        "# decode sequence\n",
        "result = beam_search_decoder(data, 3)\n",
        "\n",
        "\n",
        "print()\n",
        "print(\"The final decoded 3 best sequences: \")\n",
        "for seq in result:\n",
        "    print(seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 3 best sequences at step  0 : \n",
            "[[[3], 0.916290731874155], [[2], 1.2039728043259361], [[4], 1.6094379124341003]]\n",
            "\n",
            "The 3 best sequences at step  1 : \n",
            "[[[3, 0], 1.83258146374831], [[3, 1], 2.120263536200091], [[2, 0], 2.120263536200091]]\n",
            "\n",
            "The 3 best sequences at step  2 : \n",
            "[[[3, 0, 3], 2.748872195622465], [[3, 0, 2], 3.036554268074246], [[3, 1, 3], 3.036554268074246]]\n",
            "\n",
            "The 3 best sequences at step  3 : \n",
            "[[[3, 0, 3, 0], 3.66516292749662], [[3, 0, 3, 1], 3.952844999948401], [[3, 0, 2, 0], 3.952844999948401]]\n",
            "\n",
            "The 3 best sequences at step  4 : \n",
            "[[[3, 0, 3, 0, 3], 4.581453659370775], [[3, 0, 3, 0, 2], 4.869135731822556], [[3, 0, 3, 1, 3], 4.869135731822556]]\n",
            "\n",
            "The 3 best sequences at step  5 : \n",
            "[[[3, 0, 3, 0, 3, 0], 5.49774439124493], [[3, 0, 3, 0, 3, 1], 5.7854264636967105], [[3, 0, 3, 0, 2, 0], 5.785426463696711]]\n",
            "\n",
            "The 3 best sequences at step  6 : \n",
            "[[[3, 0, 3, 0, 3, 0, 3], 6.414035123119085], [[3, 0, 3, 0, 3, 0, 2], 6.701717195570866], [[3, 0, 3, 0, 3, 1, 3], 6.701717195570866]]\n",
            "\n",
            "The 3 best sequences at step  7 : \n",
            "[[[3, 0, 3, 0, 3, 0, 3, 0], 7.33032585499324], [[3, 0, 3, 0, 3, 0, 3, 1], 7.618007927445021], [[3, 0, 3, 0, 3, 0, 2, 0], 7.618007927445021]]\n",
            "\n",
            "The 3 best sequences at step  8 : \n",
            "[[[3, 0, 3, 0, 3, 0, 3, 0, 3], 8.246616586867395], [[3, 0, 3, 0, 3, 0, 3, 1, 3], 8.534298659319175], [[3, 0, 3, 0, 3, 0, 2, 0, 3], 8.534298659319175]]\n",
            "\n",
            "The 3 best sequences at step  9 : \n",
            "[[[3, 0, 3, 0, 3, 0, 3, 0, 3, 0], 9.16290731874155], [[3, 0, 3, 0, 3, 0, 3, 1, 3, 0], 9.45058939119333], [[3, 0, 3, 0, 3, 0, 2, 0, 3, 0], 9.45058939119333]]\n",
            "\n",
            "\n",
            "The final decoded 3 best sequences: \n",
            "[[3, 0, 3, 0, 3, 0, 3, 0, 3, 0], 9.16290731874155]\n",
            "[[3, 0, 3, 0, 3, 0, 3, 1, 3, 0], 9.45058939119333]\n",
            "[[3, 0, 3, 0, 3, 0, 2, 0, 3, 0], 9.45058939119333]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRivr820ULSk"
      },
      "source": [
        "#Neural Language Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egk2F3luQI5g"
      },
      "source": [
        "Now, let's see how to build a language model for generating natural language text by implement and training a Recurrent Neural Network. The objective of this model is to generate new text, given that some input text is present. Let's start building the architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKTEsdU_ULSm"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from numpy import log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFIkyodwQO6d"
      },
      "source": [
        "Let's use a popular nursery rhyme — “Cat and Her Kittens” as our corpus.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhy-wko0TdkP"
      },
      "source": [
        "import re\n",
        "\n",
        "# Pad sequences to the max length\n",
        "def pad_sequences_pre(input_sequences, maxlen):\n",
        "    output = []\n",
        "    for inp in input_sequences:\n",
        "        if len(inp)< maxlen:\n",
        "            output.append([0]*(maxlen-len(inp)) + inp)\n",
        "        else:\n",
        "            output.append(inp[:maxlen])\n",
        "    return output\n",
        "\n",
        "# Prepare the data\n",
        "def dataset_preparation(data):\n",
        "    corpus = data.lower().split(\"\\n\")\n",
        "    normalized_text=[]\n",
        "    for string in corpus:\n",
        "        tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "        normalized_text.append(tokens)\n",
        "    tokenized_sentences=[sentence.strip().split(\" \") for sentence in normalized_text]\n",
        "\n",
        "    word_list_dict ={}\n",
        "    for sent in tokenized_sentences:\n",
        "        for word in sent:\n",
        "            if word != \"\":\n",
        "                word_list_dict[word] = 1\n",
        "    word_list = list(word_list_dict.keys())\n",
        "    word_to_index = {word:word_list.index(word) for word in word_list}\n",
        "\n",
        "    total_words = len(word_list)+1\n",
        "\n",
        "    # create input sequences using list of tokens\n",
        "    input_sequences = []\n",
        "    for line in tokenized_sentences:\n",
        "        token_list = []\n",
        "        for word in line:\n",
        "            if word!=\"\":\n",
        "                token_list.append(word_to_index[word])\n",
        "        for i in range(1, len(token_list)):\n",
        "            n_gram_sequence = token_list[:i+1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "\n",
        "    # pad sequences\n",
        "    max_sequence_len = max([len(x) for x in input_sequences])\n",
        "    input_sequences = np.array(pad_sequences_pre(input_sequences, maxlen=max_sequence_len))\n",
        "\n",
        "    # create predictors and label\n",
        "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "    return predictors, np.array(label), max_sequence_len, total_words, word_list, word_to_index\n",
        "\n",
        "data = '''The cat and her kittens\n",
        "They put on their mittens\n",
        "To eat a christmas pie\n",
        "The poor little kittens\n",
        "They lost their mittens\n",
        "And then they began to cry.\n",
        "\n",
        "O mother dear, we sadly fear\n",
        "We cannot go to-day,\n",
        "For we have lost our mittens\n",
        "If it be so, ye shall not go\n",
        "For ye are naughty kittens'''\n",
        "\n",
        "predictors, label, max_sequence_len, total_words, word_list, word_to_index = dataset_preparation(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCDUDPold5T8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "e0bf8137-b4e0-4003-a7cc-9e24e098e90c"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the model\n",
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim_1, hidden_dim_2, total_words):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim_1 = hidden_dim_1\n",
        "        self.hidden_dim_2 = hidden_dim_2\n",
        "        self.word_embeddings = nn.Embedding(total_words, embedding_dim)\n",
        "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim_1, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(hidden_dim_1, hidden_dim_2, batch_first=True)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim_2, total_words)\n",
        "\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out_1, _ = self.lstm1(embeds)\n",
        "        lstm_out_2, _ = self.lstm2(lstm_out_1)\n",
        "        tag_space = self.hidden2tag(lstm_out_2[:,-1,:])\n",
        "        # The reason we are using log_softmax here is that we want to calculate -log(p) and find the minimum score\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores\n",
        "\n",
        "# Parameter setting\n",
        "EMBEDDING_DIM = 10\n",
        "HIDDEN_DIM_1 = 150\n",
        "HIDDEN_DIM_2 = 100\n",
        "batch_size=predictors.shape[0]\n",
        "\n",
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM_1, HIDDEN_DIM_2, total_words).cuda()\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "sentence =torch.from_numpy(predictors).cuda().to(torch.int64)\n",
        "targets = torch.from_numpy(label).cuda().to(torch.int64)\n",
        "\n",
        "\n",
        "# Training\n",
        "for epoch in range(100):\n",
        "\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    tag_scores = model(sentence)\n",
        "    loss = loss_function(tag_scores, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    if epoch % 10 == 9:\n",
        "        model.eval()\n",
        "        _, predicted = torch.max(tag_scores, 1)\n",
        "        prediction = predicted.view(-1).cpu().numpy()\n",
        "        t = targets.view(-1).cpu().numpy()\n",
        "        acc = accuracy_score(prediction,t)\n",
        "        print('Epoch: %d, training loss: %.4f, training acc: %.2f%%'%(epoch+1,loss.item(),100*acc))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-812d72eeb41b>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTMTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_DIM_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_DIM_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m         \"\"\"\n\u001b[0;32m--> 905\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m         \"\"\"\n\u001b[0;32m--> 905\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'LAZY'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy_B1EX4QRPS"
      },
      "source": [
        "For decoding, let's first practice with beam search with k=1, which is equivalent to greedy decoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOpunEmZNoLZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c537057-41b9-424e-dc3e-2df4975526b0"
      },
      "source": [
        "# convert index to word\n",
        "def ind_to_word(predicted_ind):\n",
        "    for word, index in word_to_index.items():\n",
        "        if index == predicted_ind:\n",
        "            return word\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "# get the top k most predicted results\n",
        "def get_topK(predicted, k=1):\n",
        "\n",
        "    # Get the index of the highest k index\n",
        "    # Since the input is just one sentence, we can use [0] to extract the prediction result\n",
        "    top_k = np.argsort(predicted[0])[-k:]\n",
        "\n",
        "    # return a list of tuple\n",
        "    # tuple[0]:word_id, tuple[1]:log(p)\n",
        "    return [(id, predicted[0][id]) for id in top_k]\n",
        "\n",
        "\n",
        "\n",
        "# Generate text, currently it only works with k=1\n",
        "def generate_text(seed_text, next_words, max_sequence_len, k=1):\n",
        "\n",
        "    seed_candidates = [(seed_text, .0)]\n",
        "    for _ in range(next_words):\n",
        "        successives = []\n",
        "        # if k = 1, len(seed_candidates) will always be 1\n",
        "        for i in range(len(seed_candidates)):\n",
        "            seed_text, score = seed_candidates[i]\n",
        "            token_list = [word_to_index[word] for word in seed_text.split()] # every time put a current sentence as the input\n",
        "            token_list = pad_sequences_pre([token_list], maxlen=max_sequence_len-1)\n",
        "\n",
        "            seed_input = torch.from_numpy(np.array(token_list)).cuda().to(torch.int64)\n",
        "            predicted = model(seed_input).cpu().detach().numpy()\n",
        "\n",
        "\n",
        "            # Since it only works with k = 1, we can simply use [0] to get the word id and log(p)-->here is a greedy search\n",
        "            id, s = get_topK(predicted, k)[0]\n",
        "            # get the output word\n",
        "            output_word = ind_to_word(id)\n",
        "            # put the word into the sentence input\n",
        "            # calcualte the accumulated score by -log(p)\n",
        "            successives.append((seed_text + ' ' + output_word, score - s))\n",
        "\n",
        "        # Get the lowest k accumulated scores (highest k accumulated probabilities)\n",
        "        # Then, make them as the seed_candidate for the next word to predict\n",
        "        ordered = sorted(successives, key=lambda tup: tup[1])\n",
        "        seed_candidates = ordered[:k]\n",
        "\n",
        "    return seed_candidates[0][0]\n",
        "\n",
        "\n",
        "print(generate_text(\"we naughty\", 3, max_sequence_len, k=1))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we naughty go to day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUYNu7WBt4V-"
      },
      "source": [
        "Now, let's modify based on the above code to allow k>1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk05en7NnuiN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ed3cb1a-2ea1-45d4-aaed-d01ce03826d1"
      },
      "source": [
        "def generate_text(seed_text, next_words, max_sequence_len, k=1):\n",
        "\n",
        "    seed_candidates = [(seed_text, .0)]\n",
        "    for _ in range(next_words):\n",
        "        successives = []\n",
        "        for i in range(len(seed_candidates)):\n",
        "            seed_text, score = seed_candidates[i]\n",
        "            token_list = [word_to_index[word] for word in seed_text.split()]\n",
        "            token_list = pad_sequences_pre([token_list], maxlen=max_sequence_len-1)\n",
        "\n",
        "            seed_input = torch.from_numpy(np.array(token_list)).cuda().to(torch.int64)\n",
        "            predicted = model(seed_input).cpu().detach().numpy()\n",
        "\n",
        "            # if k>1 , we can't simply use [0] to get the candidates\n",
        "            # instead, we will modify as follows\n",
        "            for id, s in get_topK(predicted, k):\n",
        "                output_word= ind_to_word(id)\n",
        "                successives.append((seed_text + ' ' + output_word, score - s))\n",
        "        ordered = sorted(successives, key=lambda tup: tup[1])\n",
        "        seed_candidates = ordered[:k]\n",
        "    return seed_candidates[0][0]\n",
        "\n",
        "# Please note that it can happen that k=1 and k=3 have the same output because this is only a small dataset.\n",
        "print(generate_text(\"we naughty\", 3, max_sequence_len, k=1))\n",
        "print(generate_text(\"we naughty\", 3, max_sequence_len, k=3))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we naughty go to day\n",
            "we naughty go to day\n"
          ]
        }
      ]
    }
  ]
}